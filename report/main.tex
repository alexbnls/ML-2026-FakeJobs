\documentclass[12pt,a4paper,oneside]{report}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[italian]{babel}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{titlesec}

% Setup Geometria
\geometry{
    left=3cm,
    right=2.5cm,
    top=3cm,
    bottom=3cm,
    headheight=1.5cm,
    headsep=0.5cm,
    footskip=1.5cm
}

% Header e Footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small \textit{Fraud Job Postings Detection}}
\fancyhead[R]{\small A.A. 2024/2025}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% Stile Codice Python
\definecolor{commentgray}{gray}{0.35}
\definecolor{lightgray}{gray}{0.95}

\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries\color{black},
    stringstyle=\color{gray},
    commentstyle=\itshape\color{commentgray},
    numberstyle=\tiny\color{gray},
    numbers=left,
    numbersep=10pt,
    backgroundcolor=\color{white},
    frame=tb,
    rulecolor=\color{gray},
    breaklines=true,
    breakatwhitespace=true,
    captionpos=b,
    showstringspaces=false,
    tabsize=4,
    keepspaces=true
}

% Setup Link
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,
    urlcolor=blue,
    pdftitle={Report Progetto ML},
}

\begin{document}

% ------------------------------------------------------------------------------
% FRONTESPIZIO
% ------------------------------------------------------------------------------
\begin{titlepage}
    \begin{center}
        
        \includegraphics[width=0.35\textwidth]{logo_unisa.png}
        \vspace{0.5cm}
        
        \LARGE
        \textbf{UNIVERSITÀ DEGLI STUDI DI SALERNO} \\
        \vspace{0.5cm}
        \Large
        Dipartimento di Informatica \\
        Corso di Machine Learning
        
        \vspace{3.5cm}
        
        \Huge
        \textbf{Rilevamento di Annunci di Lavoro Fraudolenti mediante Machine Learning} \\
        \vspace{0.5cm}
        \Large
        \textit{Fraud Job Postings Detection Pipeline}
        
        \vspace{3cm}
        
        \begin{minipage}{0.4\textwidth}
            \begin{flushleft} \large
                \textbf{Docenti:}\\
                Prof. Giuseppe Polese\\
                Prof.ssa Loredana Caruccio
            \end{flushleft}
        \end{minipage}
        \hfill
        \begin{minipage}{0.4\textwidth}
            \begin{flushright} \large
                \textbf{Studenti:} \\
                Alessandro D.\\
                Gaetano A.
            \end{flushright}
        \end{minipage}
        
        \vfill
        
        \Large
        Anno Accademico 2025/2026
        
    \end{center}
\end{titlepage}

% ------------------------------------------------------------------------------
% ABSTRACT
% ------------------------------------------------------------------------------
\chapter*{Abstract}
Questo report presenta un'analisi completa di un sistema di classificazione binaria per la rilevazione di annunci di lavoro fraudolenti. Il dataset contiene 17.880 annunci di lavoro con 18 caratteristiche, tra cui dati testuali e metadati. L'approccio proposto combina tecniche di elaborazione del linguaggio naturale (NLP) con feature engineering avanzato e ensemble methods. Il modello migliore (Random Forest) raggiunge un F1-Score di 0.7041 sul test set con threshold standard a 0.5, migliorabile a \textbf{0.8023} mediante \textit{threshold tuning} a 0.20.

\newpage

% ------------------------------------------------------------------------------
% INDICE
% ------------------------------------------------------------------------------
\tableofcontents
\newpage

% ------------------------------------------------------------------------------
% CAPITOLO 1: INTRODUZIONE
% ------------------------------------------------------------------------------
\chapter{Introduzione}

\section{Contesto e Motivazione}
Il fenomeno delle truffe negli annunci di lavoro online è divenuto sempre più pervasivo negli ultimi anni, esponendo aziende, piattaforme di recruiting e candidati a rischi crescenti. Le modalità di attacco sono eterogenee e spaziano da campagne di phishing mirate al furto di identità, fino a complesse frodi finanziarie. 

In tale scenario, lo sviluppo di un sistema automatico di rilevazione risulta fondamentale sotto molteplici aspetti. Primariamente, esso funge da barriera per \textbf{proteggere i candidati}, evitando che vengano esposti a tentativi di frode che potrebbero comprometterne la sicurezza dei dati personali ed economici. Parallelamente, l'automazione contribuisce a \textbf{mantenere la fiducia} nell'ecosistema del recruiting, garantendo la credibilità delle piattaforme che ospitano le offerte. Dal punto di vista aziendale, l'adozione di modelli predittivi permette di \textbf{ridurre significativamente i costi operativi} legati alla moderazione manuale, consentendo al contempo di \textbf{analizzare i pattern di frode} emergenti per comprendere e anticipare l'evoluzione delle strategie degli attaccanti.

\section{Obiettivi del Progetto}
L'obiettivo primario del progetto è lo sviluppo di una pipeline completa di Machine Learning dedicata alla classificazione binaria degli annunci. Il lavoro prende avvio dall'esplorazione e dal preprocessamento di un dataset reale, affrontando sfide critiche come la gestione di una notevole quantità di valori mancanti. 

Nello specifico, l'approccio metodologico punta su un \textbf{feature engineering avanzato}, che combina tecniche di Natural Language Processing (NLP) per l'analisi dei testi con l'estrazione di metadati numerici e categorici. La fase sperimentale prevede l'addestramento e il confronto di diversi modelli di classificazione, la cui efficacia viene massimizzata attraverso tecniche di ottimizzazione come il \textit{threshold tuning}. Infine, grande enfasi viene posta sull'analisi dei risultati e sull'interpretabilità dei modelli, elemento essenziale per comprenderne le decisioni in un contesto di sicurezza informatica.

\section{Struttura del Report}
Il report è organizzato come segue:
\begin{description}
    \item[Capitolo 2] Analisi Esplorativa dei Dati (EDA)
    \item[Capitolo 3] Data Cleaning e Preprocessing
    \item[Capitolo 4] Feature Engineering
    \item[Capitolo 5] Preparazione Features e Splitting
    \item[Capitolo 6] Modelli e Metodologie di Training
    \item[Capitolo 7] Risultati e Valutazione
    \item[Capitolo 8] Analisi Dettagliata del Miglior Modello
    \item[Capitolo 9] Conclusioni e Lavori Futuri
\end{description}

% ------------------------------------------------------------------------------
% CAPITOLO 2: EDA
% ------------------------------------------------------------------------------
\chapter{Analisi Esplorativa dei Dati (EDA)}

\section{Descrizione Dataset}
Il dataset contiene informazioni su annunci di lavoro provenienti da una piattaforma di recruiting online. Le statistiche principali sono:

\begin{table}[H]
\centering
\caption{Statistiche Descrittive del Dataset}
\begin{tabular}{lc}
\toprule
\textbf{Metrica} & \textbf{Valore} \\
\midrule
Numero di campioni & 17.880 \\
Numero di caratteristiche & 18 \\
Numero di annunci reali & 17.014 (95.16\%) \\
Numero di annunci fraudolenti & 866 (4.84\%) \\
\bottomrule
\end{tabular}
\end{table}

\section{Descrizione Features}
Il dataset è caratterizzato da una struttura eterogenea che combina dati non strutturati (testo libero) con metadati strutturati. Le variabili a disposizione possono essere suddivise in tre macro-categorie principali in base alla loro natura e al trattamento necessario in fase di preprocessing.

\subsection{Features Testuali}
Le variabili testuali costituiscono la parte più ricca di informazioni semantiche e richiedono tecniche di Natural Language Processing (NLP) per essere analizzate. Il nucleo dell'annuncio è rappresentato dal \texttt{title} e dalla \texttt{description}, che forniscono i dettagli essenziali sulla posizione aperta. A questi si affiancano campi specifici che delineano il contesto aziendale (\texttt{company\_profile}), le competenze necessarie per la candidatura (\texttt{requirements}) e gli eventuali vantaggi offerti al dipendente (\texttt{benefits}).

\subsection{Features Categoriche}
Le variabili categoriche forniscono il contesto strutturale e classificatorio dell'offerta di lavoro. La dimensione geografica e organizzativa è catturata rispettivamente dalle feature \texttt{location} e \texttt{department}. Per quanto riguarda i dettagli contrattuali e professionali, il dataset include informazioni sulla tipologia di impiego (\texttt{employment\_type}), sul livello di esperienza richiesto (\texttt{required\_experience}) e sul grado di istruzione necessario (\texttt{required\_education}). Infine, il posizionamento di mercato dell'azienda è definito dal settore industriale di appartenenza (\texttt{industry}) e dalla specifica funzione lavorativa (\texttt{function}).

\subsection{Features Numeriche e Binarie}
Quest'ultima categoria include indicatori diretti e metadati specifici. L'unica variabile contenente informazioni quantitative sull'offerta economica è \texttt{salary\_range}, che specifica l'intervallo retributivo previsto. Le restanti feature sono indicatori binari (flag) che segnalano la presenza di determinate caratteristiche: la possibilità di lavorare da remoto (\texttt{telecommuting}), la presenza del logo aziendale nell'annuncio (\texttt{has\_company\_logo}) e l'esistenza di un questionario di screening per i candidati (\texttt{has\_questions}).

\section{Analisi Missing Values}
Il dataset presenta un numero significativo di valori mancanti. La distribuzione è la seguente:

\begin{table}[H]
\centering
\caption{Percentuale Missing Values per Colonna}
\begin{tabular}{lr}
\toprule
\textbf{Colonna} & \textbf{Missing (\%)} \\
\midrule
salary\_range & 83.96\% \\
department & 64.58\% \\
required\_education & 45.33\% \\
benefits & 40.34\% \\
required\_experience & 39.43\% \\
function & 36.10\% \\
industry & 27.42\% \\
employment\_type & 19.41\% \\
company\_profile & 18.50\% \\
requirements & 15.08\% \\
location & 1.94\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{02_missing_values.png}
    \caption{Visualizzazione grafica dei valori mancanti nel dataset.}
    \label{fig:missing}
\end{figure}

La percentuale di missing values è molto elevata in alcune colonne (ad es. \texttt{salary\_range} con 83.96\%). Questo richiede una strategia di imputazione ben calibrata per non perdere informazioni critiche.

\section{Distribuzione Target}
Il dataset presenta un forte sbilanciamento di classe:

\begin{table}[H]
\centering
\caption{Distribuzione Classe Target}
\begin{tabular}{lcc}
\toprule
\textbf{Classe} & \textbf{Conteggio} & \textbf{Percentuale} \\
\midrule
Reale (0) & 17.014 & 95.16\% \\
Fraudolento (1) & 866 & 4.84\% \\
\midrule
\textbf{Totale} & 17.880 & 100\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{01_class_distribution.png}
    \caption{Sbilanciamento delle classi: Annunci Reali vs Fraudolenti.}
    \label{fig:class_dist}
\end{figure}

Lo sbilanciamento significativo (rapporto 19.65:1) richiede l'uso di tecniche specializzate durante l'addestramento e la valutazione:
\begin{itemize}
    \item Stratified train-test split
    \item Class weight balancing nei modelli
    \item Metriche appropriate (F1-Score, Precision-Recall, ROC-AUC)
\end{itemize}

% ------------------------------------------------------------------------------
% CAPITOLO 3: DATA CLEANING
% ------------------------------------------------------------------------------
\chapter{Data Cleaning e Preprocessing}

\section{Gestione Missing Values}
La strategia adottata è differenziata per tipologia di feature:

\subsection{Features Testuali}
Tutte le features testuali con valori nulli sono state riempite con stringhe vuote (\texttt{''}). Questo approccio preserva la structure del vettore TF-IDF e permette al modello di imparare che l'assenza di testo è un segnale importante.

\begin{lstlisting}[language=Python, caption=Riempimento Missing Values Testuali]
text_cols = ['company_profile', 'description', 'requirements', 'benefits']
for col in text_cols:
    df_clean[col] = df_clean[col].fillna('')
\end{lstlisting}

\subsection{Features Categoriche}
Le features categoriche sono state riempite con il placeholder \texttt{'Unknown'}, permettendo al modello di trattare i valori mancanti come una categoria a parte.

\begin{lstlisting}[language=Python, caption=Riempimento Missing Values Categorici]
category_cols = ['department', 'employment_type', 'required_experience', 
                 'required_education', 'industry', 'function', 'country']
for col in category_cols:
    df_clean[col] = df_clean[col].fillna('Unknown')
\end{lstlisting}

\section{Pulizia Testuale}
Le features testuali sono state sottoposte a un processo di normalizzazione:

\begin{lstlisting}[language=Python, caption=Processo di Pulizia Testo]
def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = text.lower()                     # Minuscole
    text = re.sub(r'<.*?>', '', text)       # Rimozione tag HTML
    text = re.sub(r'http\S+', '', text)     # Rimozione URL
    text = re.sub(r'[^a-zA-Z\s]', '', text) # Rimozione numeri e punteggiatura
    text = re.sub(r'\s+', ' ', text).strip()# Normalizzazione spazi
    return text
\end{lstlisting}

Questa pulizia serve a:
\begin{enumerate}
    \item Ridurre il rumore (URL, tag HTML, punteggiatura).
    \item Normalizzare il testo (minuscole, spazi multipli).
    \item Preparare il testo per la TF-IDF vectorization.
\end{enumerate}

\section{Estrazione di Salary Information}
Dalla colonna \texttt{salary\_range} sono stati estratti tre nuovi segnali:

\begin{lstlisting}[language=Python, caption=Estrazione Informazioni Salariali]
def extract_salary_info(salary_str):
    if pd.isna(salary_str) or salary_str == '':
        return 0, 0
    try:
        nums = re.findall(r'\d+', str(salary_str))
        if len(nums) >= 2:
            return float(nums[0]), float(nums[1])
        elif len(nums) == 1:
            return float(nums[0]), float(nums[0])
    except:
        pass
    return 0, 0

df_clean[['salary_min', 'salary_max']] = \
    df_clean['salary_range'].apply(lambda x: pd.Series(extract_salary_info(x)))
    
df_clean['salary_range_flag'] = \
    (df_clean['salary_range'].notna() & (df_clean['salary_range'] != '')).astype(int)
\end{lstlisting}

Le tre features risultanti sono:
\begin{itemize}
    \item \texttt{salary\_min}: Salario minimo proposto.
    \item \texttt{salary\_max}: Salario massimo proposto.
    \item \texttt{salary\_range\_flag}: Flag binario per presenza di salary range.
\end{itemize}

\section{Estrazione Location - Feature Engineering}
Dalla colonna \texttt{location} (es. "US, NY, New York") è stato estratto il paese:

\begin{lstlisting}[language=Python, caption=Estrazione Paese dalla Location]
df_clean['country'] = df_clean['location'].apply(
    lambda x: x.split(',')[0].strip() if isinstance(x, str) and ',' in x 
              else 'Unknown'
)
\end{lstlisting}

Questa riduzione di cardinalità è strategica poiché:
\begin{enumerate}
    \item Reduce overfitting su location troppo specifiche.
    \item Consente analisi geografica dei pattern di frode.
    \item Riduce dimensionalità del feature set.
\end{enumerate}

% ------------------------------------------------------------------------------
% CAPITOLO 4: FEATURE ENGINEERING
% ------------------------------------------------------------------------------
\chapter{Feature Engineering}

\section{Meta-Features Testuali}
È stata creata una feature testuale combinata:

\begin{lstlisting}[language=Python, caption=Creazione Combined Text]
df_clean['combined_text'] = (
    df_clean['description'] + ' ' +
    df_clean['requirements'] + ' ' +
    df_clean['benefits']
)
\end{lstlisting}

Sulla quale verrà applicato il TF-IDF vectorization.

\section{Features di Lunghezza e Conteggio}
Sono state calcolate features di lunghezza e conteggio di parole:

\begin{table}[H]
\centering
\caption{Meta-Features Testuali Estratte}
\begin{tabular}{ll}
\toprule
\textbf{Feature} & \textbf{Descrizione} \\
\midrule
len\_description & Lunghezza in caratteri della descrizione \\
len\_requirements & Lunghezza in caratteri dei requisiti \\
len\_benefits & Lunghezza in caratteri dei benefit \\
len\_company\_profile & Lunghezza in caratteri del profilo aziendale \\
words\_description & Numero di parole nella descrizione \\
words\_requirements & Numero di parole nei requisiti \\
\bottomrule
\end{tabular}
\end{table}

Queste features catturano il concetto che gli annunci fraudolenti tendono ad avere lunghezze anomale (molto brevi o molto lunghi).

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{04_features_distribution.png}
    \caption{Distribuzione delle nuove features create (lunghezza testi e conteggio parole).}
    \label{fig:feat_dist}
\end{figure}

\section{Gestione degli Outliers: Log-Transformation}
Le distribuzioni di lunghezza e salario presentano code lunghe (skewed distributions). È stata applicata una trasformazione logaritmica:

\begin{lstlisting}[language=Python, caption=Log-Transformation]
numeric_features = ['len_description', 'len_requirements', 'len_benefits',
                    'len_company_profile', 'words_description', 'words_requirements',
                    'salary_min', 'salary_max']

for col in numeric_features:
    df_clean[f'log_{col}'] = np.log1p(df_clean[col])
\end{lstlisting}

La trasformazione $\log(1+x)$ normalizza la distribuzione e riduce l'impatto degli outlier, migliorando la stabilità dei modelli lineari e tree-based.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{03_outliers_management.png}
    \caption{Confronto prima e dopo la trasformazione logaritmica per la gestione degli outliers.}
    \label{fig:outliers}
\end{figure}

\section{Encoding delle Features Categoriche}
Per le 6 features categoriche è stato utilizzato LabelEncoder:

\begin{table}[H]
\centering
\caption{Features Categoriche Encodate}
\begin{tabular}{llc}
\toprule
\textbf{Feature Originale} & \textbf{Feature Encodata} & \textbf{Cardinalità} \\
\midrule
employment\_type & employment\_type\_encoded & $\sim$ 5 \\
required\_experience & required\_experience\_encoded & $\sim$ 6 \\
required\_education & required\_education\_encoded & $\sim$ 7 \\
industry & industry\_encoded & $\sim$ 50 \\
function & function\_encoded & $\sim$ 30 \\
country & country\_encoded & $\sim$ 100 \\
\bottomrule
\end{tabular}
\end{table}

LabelEncoder è stato scelto al posto di OneHotEncoder perché:
\begin{enumerate}
    \item È ottimale per modelli tree-based (Random Forest).
    \item Riduce significativamente la dimensionalità.
    \item Il dataset è già molto dimensionale (5000+ features TF-IDF).
\end{enumerate}

% ------------------------------------------------------------------------------
% CAPITOLO 5: PREPARAZIONE E SPLITTING
% ------------------------------------------------------------------------------
\chapter{Preparazione Dati e Splitting}

\section{Stratified Train-Test Split}
Il dataset è stato diviso usando stratified split per mantenere la stessa proporzione di frodi in train e test:

\begin{table}[H]
\centering
\caption{Suddivisione Train-Test}
\begin{tabular}{lccc}
\toprule
\textbf{Set} & \textbf{Campioni} & \textbf{Reali} & \textbf{Fraudolenti} \\
\midrule
Training & 14.304 & 13.611 (95.16\%) & 693 (4.84\%) \\
Test & 3.576 & 3.403 (95.16\%) & 173 (4.84\%) \\
\bottomrule
\end{tabular}
\end{table}

La proporzione è perfettamente bilanciata, confermando l'efficacia della stratificazione.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{05_train_test_split.png}
    \caption{Verifica della stratificazione: la proporzione delle classi è mantenuta tra Train e Test.}
    \label{fig:split}
\end{figure}

\section{TF-IDF Vectorization}
Il testo combinato è stato vettorializzato usando TF-IDF con i seguenti parametri:

\begin{table}[H]
\centering
\caption{Parametri TF-IDF}
\begin{tabular}{lll}
\toprule
\textbf{Parametro} & \textbf{Valore} & \textbf{Motivazione} \\
\midrule
max\_features & 5000 & Cattura vocab sufficiente senza overfitting \\
stop\_words & 'english' & Rimozione parole comuni \\
ngram\_range & (1, 2) & Cattura unigrammi e bigrammi \\
min\_df & 5 & Ignora token molto rari \\
max\_df & 0.7 & Ignora token troppo frequenti \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{06_tfidf_features.png}
    \caption{Rappresentazione delle features testuali più frequenti estratte tramite TF-IDF.}
    \label{fig:tfidf}
\end{figure}

Risultato: \textbf{5000 features TF-IDF} dalla vettorializzazione del testo.

\section{Feature Scaling}
Le meta-features numeriche sono state scalate usando RobustScaler:

\begin{lstlisting}[language=Python, caption=Scaling con RobustScaler]
scaler = RobustScaler()
X_meta_train_scaled = scaler.fit_transform(X_meta_train)
X_meta_test_scaled = scaler.transform(X_meta_test)
\end{lstlisting}

RobustScaler è stato scelto perché:
\begin{enumerate}
    \item Usa mediana e IQR anziché media e std.
    \item È robusto agli outlier.
    \item Preferibile a StandardScaler su dati skewed.
\end{enumerate}

\section{Feature Concatenation}
Le features finali sono create concatenando:
\begin{enumerate}
    \item TF-IDF features (5000)
    \item Log-transformed numeric features (8)
    \item Binary features (4)
    \item Encoded categorical features (6)
\end{enumerate}

\textbf{Dimensione finale: 5018 features per 14.304 campioni di training.}

% ------------------------------------------------------------------------------
% CAPITOLO 6: MODELLI
% ------------------------------------------------------------------------------
\chapter{Modelli e Metodologie}

\section{Modelli Addestrati}
La fase di sperimentazione ha coinvolto la selezione e l'addestramento di tre algoritmi di classificazione distinti. La scelta è stata guidata dalla necessità di confrontare approcci lineari (veloci e interpretabili) con metodi ensemble (robusti e non lineari), ponendo particolare attenzione alla gestione dell'alta dimensionalità derivante dal TF-IDF e allo sbilanciamento delle classi.

\subsection{Logistic Regression}
Come baseline di riferimento è stata selezionata la Regressione Logistica, apprezzata per la sua velocità di training e per l'interpretabilità diretta dei coefficienti, che permettono di comprendere l'impatto di ogni feature sulla predizione. 
La configurazione del modello ha previsto l'utilizzo del solver \texttt{'lbfgs'} e un numero massimo di iterazioni pari a 1000 (\texttt{max\_iter=1000}) per garantire la convergenza anche in uno spazio vettoriale complesso. L'aspetto cruciale della configurazione risiede nel parametro \texttt{class\_weight='balanced'}: questa impostazione adatta automaticamente i pesi della funzione di costo in modo inversamente proporzionale alla frequenza delle classi, penalizzando maggiormente gli errori sulla classe minoritaria (le frodi).

\subsection{Random Forest}
Per catturare le relazioni non lineari e le interazioni complesse tra le feature (es. tra testo e metadati), è stato implementato un modello ensemble di tipo Random Forest. Questo algoritmo, basato sul bagging di alberi decisionali, offre una naturale robustezza agli outlier e riduce il rischio di overfitting rispetto a un singolo albero.
Il modello è stato istanziato con 100 stimatori (\texttt{n\_estimators=100}) e, per permettere al modello di apprendere pattern specifici e profondi, non è stato imposto alcun limite alla profondità degli alberi (\texttt{max\_depth=None}). Anche in questo caso, è stato applicato il bilanciamento dei pesi (\texttt{class\_weight='balanced'}) per contrastare la disparità numerica tra annunci reali e fraudolenti. L'esecuzione è stata parallelizzata (\texttt{n\_jobs=-1}) per ottimizzare i tempi di calcolo.

\subsection{Linear SVM (SGD Classifier)}
Considerata l'alta dimensionalità dello spazio delle feature (oltre 5000 dimensioni), le Support Vector Machines (SVM) rappresentano una scelta teoricamente solida. Per garantire la scalabilità su un dataset di queste dimensioni, si è optato per un'implementazione basata sulla Discesa del Gradiente Stocastico (SGD).
Il classificatore è stato configurato con la funzione di perdita \texttt{loss='log\_loss'}, che rende il modello matematicamente equivalente a una regressione logistica ma ottimizzata via SGD, permettendo di ottenere un output probabilistico nativo. Come per gli altri modelli, il parametro \texttt{class\_weight='balanced'} è stato fondamentale per orientare l'iperpiano di separazione in modo da favorire la corretta classificazione della classe minoritaria.

\section{Metriche di Valutazione}
La valutazione di un classificatore in un contesto di \textit{Fraud Detection} richiede un'attenzione particolare alla scelta delle metriche. A causa della natura fortemente sbilanciata del dataset (dove le frodi rappresentano meno del 5\% dei dati), la semplice \textbf{Accuracy} risulta una metrica ingannevole: un modello banale che classificasse tutti gli annunci come "reali" otterrebbe un'accuratezza superiore al 95\%, pur fallendo completamente l'obiettivo del progetto.

Di conseguenza, l'analisi si è concentrata sulle metriche derivate dalla Matrice di Confusione, privilegiando quelle che isolano la capacità del modello di rilevare la classe positiva (frodi):

\begin{itemize}
    \item La \textbf{Precision} misura l'affidabilità degli allarmi generati (quante delle frodi segnalate sono vere frodi).
    \item La \textbf{Recall} (o Sensitivity) misura la copertura del sistema (quante delle frodi totali sono state intercettate).
\end{itemize}

La metrica primaria per la selezione del modello è l'\textbf{F1-Score}, media armonica di Precision e Recall, che penalizza i modelli sbilanciati verso una sola delle due direzioni. Infine, l'area sotto la curva ROC (\textbf{ROC-AUC}) è stata utilizzata per valutare la capacità discriminativa globale del modello, indipendentemente dalla soglia di decisione scelta.

\begin{table}[H]
\centering
\caption{Formulazione delle Metriche Utilizzate}
\begin{tabular}{ll}
\toprule
\textbf{Metrica} & \textbf{Formula} \\
\midrule
Accuracy & $\frac{TP + TN}{TP + TN + FP + FN}$ \\
\addlinespace
Precision & $\frac{TP}{TP + FP}$ \\
\addlinespace
Recall & $\frac{TP}{TP + FN}$ \\
\addlinespace
F1-Score & $2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$ \\
\bottomrule
\end{tabular}
\end{table}

% ------------------------------------------------------------------------------
% CAPITOLO 7: RISULTATI
% ------------------------------------------------------------------------------
\chapter{Risultati e Valutazione}

\section{Performance dei Modelli}
I tre modelli sono stati addestrati e valutati sul test set. I risultati sono riassunti nella Tabella \ref{tab:model_comparison}:

\begin{table}[H]
\centering
\caption{Confronto Performance Modelli (Threshold = 0.5)}
\label{tab:model_comparison}
\begin{tabular}{lccccc}
\toprule
\textbf{Modello} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{ROC-AUC} \\
\midrule
Random Forest & 0.9779 & 1.0000 & 0.5434 & \textbf{0.7041} & 0.9884 \\
Logistic Regression & 0.9539 & 0.5135 & 0.8786 & 0.6482 & 0.9814 \\
Linear SVM (SGD) & 0.9393 & 0.4379 & 0.8960 & 0.5882 & 0.9770 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{07_models_comparison.png}
    \caption{Confronto grafico delle metriche (Accuracy, Precision, Recall, F1) tra i tre modelli.}
    \label{fig:model_comp}
\end{figure}

\vspace{0.5cm}
Dall'analisi comparativa emerge chiaramente la superiorità del \textbf{Random Forest}, che ottiene la performance complessiva migliore con un F1-Score di 0.7041. Questo modello dimostra un comportamento estremamente conservativo: raggiunge una precisione perfetta (1.0), indicando l'assenza totale di falsi allarmi, ma a costo di una recall moderata (0.5434), lasciando non rilevate quasi la metà delle frodi. 
Al contrario, la Regressione Logistica mostra un comportamento opposto, privilegiando la recall (0.8786) a discapito della precisione, risultando in numerosi falsi positivi. Il Linear SVM si posiziona come una via di mezzo, ma con performance globali inferiori rispetto agli altri due approcci.

\section{Analisi per Classe: Random Forest}
Per il modello vincente (Random Forest), i dettagli per classe sono:

\begin{table}[H]
\centering
\caption{Classification Report - Random Forest (Threshold 0.5)}
\begin{tabular}{lcccc}
\toprule
\textbf{Classe} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} & \textbf{Support} \\
\midrule
Reale (0) & 0.98 & 1.00 & 0.99 & 3.403 \\
Fraudolento (1) & 1.00 & 0.54 & 0.70 & 173 \\
\midrule
\textbf{Weighted Avg} & 0.98 & 0.98 & 0.98 & 3.576 \\
\bottomrule
\end{tabular}
\end{table}

Il modello è molto conservativo nel predire frodi (precision 100\%), ma perde circa metà dei casi fraudolenti (recall 54\%).

\section{Trade-off Precision-Recall}
In un contesto di rilevamento frodi, l'equilibrio tra Precision e Recall è critico e comporta scelte strategiche precise. Una precisione elevata minimizza i falsi allarmi, garantendo che ogni segnalazione sia affidabile, ma rischia di lasciar passare attività illecite. Viceversa, massimizzare la recall permette di intercettare la maggior parte delle truffe, ma può generare un "rumore" eccessivo di falsi positivi che sovraccarica gli operatori umani.

Nel caso specifico del Random Forest con soglia standard (0.5), il modello si è rivelato eccessivamente cauto: con una precisione del 100\% e una recall del 54.34\%, esso garantisce zero falsi allarmi ma fallisce nell'identificare il 45.66\% delle attività fraudolente. Questo risultato suggerisce la necessità di un intervento di ottimizzazione sulla soglia di decisione per rendere il sistema più sensibile.

% ------------------------------------------------------------------------------
% CAPITOLO 8: THRESHOLD TUNING
% ------------------------------------------------------------------------------
\chapter{Analisi Dettagliata e Threshold Tuning}

\section{Threshold Tuning}
Una scoperta cruciale è che la soglia standard di 0.5 non è ottimale per questo problema. È stato condotto uno studio sistematico di threshold tuning:

\begin{table}[H]
\centering
\caption{Threshold Tuning - Risultati Principali}
\begin{tabular}{cccc}
\toprule
\textbf{Threshold} & \textbf{F1-Score} & \textbf{Recall} & \textbf{Precision} \\
\midrule
0.50 & 0.7041 & 0.5434 & 1.0000 \\
0.45 & 0.7248 & 0.5783 & 0.9667 \\
0.40 & 0.7541 & 0.6240 & 0.9278 \\
0.35 & 0.7829 & 0.7107 & 0.8732 \\
0.30 & 0.7948 & 0.7688 & 0.8333 \\
\textbf{0.20} & \textbf{0.8023} & \textbf{0.8092} & \textbf{0.7955} \\
0.15 & 0.7836 & 0.8555 & 0.7292 \\
0.10 & 0.7386 & 0.9020 & 0.6341 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Risultato ottimale:} Con threshold = 0.20, l'F1-Score migliora a \textbf{0.8023}, un miglioramento di +0.0982 (+13.9\%).

\section{Confusion Matrix (Threshold Ottimale 0.20)}
Con il threshold ottimale, la confusion matrix è:

\begin{table}[H]
\centering
\caption{Confusion Matrix - Random Forest (Threshold 0.20)}
\begin{tabular}{lcc}
\toprule
& \textbf{Predicted Real} & \textbf{Predicted Fraud} \\
\midrule
\textbf{Actual Real} & 3.367 & 36 \\
\textbf{Actual Fraud} & 33 & 140 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{08_confusion_matrix.png}
    \caption{Matrice di Confusione del miglior modello (Random Forest).}
    \label{fig:cm}
\end{figure}

Analizzando la matrice di confusione, si osserva che il modello ha identificato correttamente 140 annunci fraudolenti (True Positives) e ben 3.367 annunci legittimi (True Negatives). Le criticità sono rappresentate dai 36 falsi allarmi (False Positives), che costituiscono un costo operativo accettabile, e soprattutto dalle 33 frodi non rilevate (False Negatives). 

Le metriche derivate confermano la robustezza della configurazione: la Specificity del 98.94\% indica un'eccellente capacità di riconoscere gli annunci reali, mentre la Sensitivity (Recall) dell'80.92\% dimostra che il sistema è ora in grado di intercettare la vasta maggioranza dei tentativi di frode, mantenendo una Precision del 79.55\%.

Metriche finali:
\begin{itemize}
    \item Specificity = $\frac{3367}{3403}$ = 0.9894 (99.24\%)
    \item Sensitivity (Recall) = $\frac{140}{173}$ = 0.8092 (80.92\%)
    \item Precision = $\frac{140}{176}$ = 0.7955 (79.55\%)
\end{itemize}

\section{Analisi delle Curve}
\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{09_roc_curve.png}
        \caption{Curva ROC}
        \label{fig:roc}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{10_precision_recall_curve.png}
        \caption{Curva Precision-Recall}
        \label{fig:pr}
    \end{subfigure}
    \caption{Analisi delle curve di performance per valutare il trade-off tra sensibilità e precisione.}
    \label{fig:curves}
\end{figure}

\subsection{ROC Curve}
La ROC curve del modello RandomForest raggiunge un'area sotto la curva (AUC) di 0.9884, indicando un'eccellente capacità discriminativa tra le due classi. L'AUC prossimo a 1.0 suggerisce che il modello è molto bravo a distinguere annunci reali da fraudolenti.

\subsection{Precision-Recall Curve}
La curva Precision-Recall mostra il trade-off tra precision (asse y) e recall (asse x). Il punto ottimale nel nostro caso è:
\begin{itemize}
    \item Recall = 0.8092
    \item Precision = 0.7955
\end{itemize}
Questo punto rappresenta un buon equilibrio tra catturare frodi e minimizzare falsi allarmi.

\section{Feature Importance}
L'analisi delle feature importance del Random Forest rivela quali caratteristiche sono più discriminative:

\begin{table}[H]
\centering
\caption{Top 15 Feature Importance}
\begin{tabular}{lc}
\toprule
\textbf{Feature} & \textbf{Importanza} \\
\midrule
log\_len\_company\_profile & 0.0461 \\
has\_company\_logo & 0.0307 \\
country\_encoded & 0.0131 \\
function\_encoded & 0.0106 \\
team (token TF-IDF) & 0.0087 \\
web (token TF-IDF) & 0.0077 \\
has\_questions & 0.0075 \\
required\_education\_encoded & 0.0071 \\
growing (token TF-IDF) & 0.0069 \\
log\_words\_requirements & 0.0068 \\
log\_len\_description & 0.0066 \\
log\_len\_requirements & 0.0066 \\
log\_words\_description & 0.0052 \\
skills (token TF-IDF) & 0.0048 \\
love (token TF-IDF) & 0.0048 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Interpretazioni principali:}
\begin{enumerate}
    \item \textbf{log\_len\_company\_profile (4.61\%)}: La lunghezza del profilo aziendale è il segnale più forte. Annunci fraudolenti tendono ad avere profili aziendali anomali.
    \item \textbf{has\_company\_logo (3.07\%)}: La presenza di un logo aziendale è il secondo segnale. Annunci fraudolenti spesso non hanno logo.
    \item \textbf{Token TF-IDF}: Parole come "team", "growing", "skills" appaiono più frequentemente in annunci legittimi.
    \item \textbf{Metadati geografici e educativi}: \texttt{country} e \texttt{required\_education} hanno importanza moderata nel distinguere frodi.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\textwidth]{11_feature_importance.png}
    \caption{Ranking delle variabili più influenti nelle decisioni del modello.}
    \label{fig:feat_imp}
\end{figure}

% ------------------------------------------------------------------------------
% CAPITOLO 9: CONCLUSIONI
% ------------------------------------------------------------------------------
\chapter{Conclusioni e Discussione}

\section{Sintesi dei Risultati}
Il progetto ha portato allo sviluppo di un sistema di rilevazione frodi efficace, dimostrando come l'integrazione di tecniche di Natural Language Processing con un'attenta ingegnerizzazione delle feature possa produrre risultati eccellenti anche su dataset complessi e sbilanciati. 
Il modello Random Forest si è confermato l'approccio vincente, raggiungendo inizialmente un F1-Score di 0.7041. Tuttavia, il vero salto di qualità è stato ottenuto attraverso il \textit{threshold tuning}: abbassando la soglia di decisione a 0.20, è stato possibile incrementare l'F1-Score a \textbf{0.8023}, portando la capacità di rilevamento delle frodi (Recall) oltre l'80\% e mantenendo al contempo un alto livello di precisione.

L'analisi dell'interpretabilità ha inoltre fatto emergere pattern chiari: le frodi si distinguono principalmente per la scarsa cura nella presentazione aziendale (profili brevi, assenza di logo) e per un vocabolario povero di termini legati alla cultura d'impresa.

\section{Limitazioni e Sviluppi Futuri}
Nonostante il successo della pipeline, permangono alcune limitazioni intrinseche. Lo squilibrio delle classi, sebbene mitigato dai pesi correttivi, rimane una sfida aperta che potrebbe beneficiare di tecniche di generazione sintetica dei dati come SMOTE. Inoltre, l'elevata percentuale di valori mancanti in colonne chiave come il salario suggerisce che strategie di imputazione più sofisticate potrebbero recuperare ulteriore segnale informativo.

Per le evoluzioni future del sistema, si raccomanda di esplorare architetture di Deep Learning basate su Transformer (es. BERT) per catturare sfumature semantiche più profonde nel testo. Sarebbe inoltre fondamentale implementare un sistema di monitoraggio continuo per validare il modello su dati temporali nuovi, prevenendo il degrado delle performance dovuto all'evoluzione delle strategie di frode. In conclusione, il sistema attuale rappresenta una solida base pronta per l'impiego in produzione, capace di filtrare efficacemente la maggior parte dei contenuti nocivi con un impatto minimo sull'operatività legittima.
\newpage

% ------------------------------------------------------------------------------
% APPENDICE: CODICE
% ------------------------------------------------------------------------------
\appendix
\chapter{Codice Principale}

\section{Pipeline Completa}
Il codice è organizzato in funzioni specializzate che si susseguono linearmente:

\begin{lstlisting}[language=Python, caption=Funzione Principale]
def main(filepath='fake_job_postings.csv'):
    # Step 1: Load & Explore
    df = load_and_explore_data(filepath)
    
    # Step 2: Clean
    df_clean = clean_data(df)
    
    # Step 3: Feature Engineering
    df_clean, cat_features, num_features = feature_engineering(df_clean)
    
    # Step 4: Prepare Features
    (X_train, X_test, y_train, y_test, vectorizer, scaler, ...) = \
        prepare_features(df_clean, cat_features, num_features)
    
    # Step 5: Train Models
    results, best_result = train_models(X_train, X_test, y_train, y_test)
    
    # Step 6: Detailed Analysis
    detailed_analysis(best_result, vectorizer, meta_features_names, y_test)
    
    return {'dataset': df_clean, 'models': results, 'best_model': best_result}
\end{lstlisting}

\chapter{Dettagli Implementativi}

\section{Gestione Classe Sbilanciata}
La stratificazione durante lo split e il \texttt{class\_weight} balancing nei modelli sono stati fondamentali:

\begin{lstlisting}[language=Python, caption=Stratified Split]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
\end{lstlisting}

\section{Parametri Modelli}
Tutti i modelli sono stati creati con:

\begin{lstlisting}[language=Python, caption=Istanzazione Modelli]
models = {
    'Logistic Regression': LogisticRegression(
        max_iter=1000, 
        random_state=42,
        class_weight='balanced',
        n_jobs=-1
    ),'Random Forest': RandomForestClassifier(
        n_estimators=100,
        random_state=42,
        class_weight='balanced',
        n_jobs=-1
    ),'Linear SVM (SGD)': SGDClassifier(
        loss='log_loss',
        random_state=42,
        class_weight='balanced',
        max_iter=1000,
        n_jobs=-1
    )
}
\end{lstlisting}

\end{document}