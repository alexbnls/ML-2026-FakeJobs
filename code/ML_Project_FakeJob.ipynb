{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Fraud Detection** (Fake Jobs)"
      ],
      "metadata": {
        "id": "kHJTuh4TY58q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import time\n",
        "import os\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             roc_auc_score, confusion_matrix, classification_report,\n",
        "                             roc_curve, auc, precision_recall_curve)\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "# Stile seaborn\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 6)"
      ],
      "metadata": {
        "id": "yQMAvKdnWgI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. CARICAMENTO E ESPLORAZIONE DATASET + GRAFICI"
      ],
      "metadata": {
        "id": "Dc1_cMnMWi53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_explore_data(filepath):\n",
        "    \"\"\"Carica e analizza il dataset\"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"STEP 1: CARICAMENTO E ESPLORAZIONE DATASET\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    df = pd.read_csv(filepath)\n",
        "\n",
        "    # 1. Analisi Struttura e Tipi (AGGIUNTO)\n",
        "    print(f\"\\nDimensioni dataset: {df.shape}\")\n",
        "    print(\"\\nInfo Dataset (Tipi di dato):\")\n",
        "    print(df.info())\n",
        "\n",
        "    # 2. Controllo Duplicati (AGGIUNTO - Molto apprezzato dai prof)\n",
        "    duplicates = df.duplicated().sum()\n",
        "    if duplicates > 0:\n",
        "        print(f\"\\n‚ö†Ô∏è ATTENZIONE: Trovati {duplicates} duplicati! Rimozione in corso...\")\n",
        "        df = df.drop_duplicates()\n",
        "        print(f\"Nuove dimensioni: {df.shape}\")\n",
        "    else:\n",
        "        print(\"\\n‚úì Nessun duplicato trovato.\")\n",
        "\n",
        "    # 3. Analisi Target\n",
        "    print(f\"\\nDistribuzione target:\")\n",
        "    print(df['fraudulent'].value_counts())\n",
        "    print(f\"Class Imbalance: {df['fraudulent'].mean()*100:.2f}% fraudulent (SBILANCIATO)\")\n",
        "\n",
        "    # 4. Analisi Missing Values\n",
        "    print(f\"\\nValori nulli per colonna (%):\")\n",
        "    missing_pct = df.isnull().sum() / len(df) * 100\n",
        "    missing_pct = missing_pct[missing_pct > 0].sort_values(ascending=False)\n",
        "    print(missing_pct)\n",
        "\n",
        "    # GRAFICO 1: Target Distribution\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Countplot\n",
        "    sns.countplot(x='fraudulent', data=df, ax=axes[0], palette=['green', 'red'])\n",
        "    axes[0].set_title('Target Distribution (Count)', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xticklabels(['Real (0)', 'Fraudulent (1)']) # Fix label\n",
        "\n",
        "    # Pie chart\n",
        "    df['fraudulent'].value_counts().plot(kind='pie', ax=axes[1],\n",
        "                                         labels=['Real', 'Fraudulent'],\n",
        "                                         autopct='%1.1f%%',\n",
        "                                         colors=['green', 'red'],\n",
        "                                         explode=(0, 0.1)) # Explode per evidenziare la slice piccola\n",
        "    axes[1].set_title('Target Distribution (%)', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_ylabel('')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('01_class_distribution.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\n‚úì Grafico salvato: 01_class_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # GRAFICO 2: Missing Values Barplot\n",
        "    if not missing_pct.empty:\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "        sns.barplot(x=missing_pct.values, y=missing_pct.index, palette='YlOrRd', ax=ax)\n",
        "        ax.set_title('Percentuale di Missing Values per Colonna', fontsize=14, fontweight='bold')\n",
        "        ax.set_xlabel('Percentuale (%)') # Pi√π chiaro di \"Number\"\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('02_missing_values.png', dpi=300, bbox_inches='tight')\n",
        "        print(\"‚úì Grafico salvato: 02_missing_values.png\")\n",
        "        plt.close()\n",
        "    else:\n",
        "        print(\"‚úì Nessun missing value da graficare.\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "K3tn2IHmWZCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. GESTIONE MISSING VALUES E DATA CLEANING"
      ],
      "metadata": {
        "id": "lQ7LhB6QWp3j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(df):\n",
        "    \"\"\"Cleaning del dataset e Preprocessing Testuale\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 2: GESTIONE MISSING VALUES E DATA CLEANING\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # 1. GESTIONE LOCATION (Feature Engineering Avanzata)\n",
        "    # Estraiamo solo il Paese (es. \"US, NY, New York\" -> \"US\")\n",
        "    # Questo riduce la cardinalit√† e aiuta il modello a trovare pattern geografici\n",
        "    df_clean['country'] = df_clean['location'].apply(\n",
        "        lambda x: x.split(',')[0].strip() if isinstance(x, str) and ',' in x else 'Unknown'\n",
        "    )\n",
        "    # Rimuoviamo la location originale che √® troppo specifica\n",
        "    df_clean = df_clean.drop('location', axis=1)\n",
        "    print(\"‚úì Feature 'country' estratta dalla location\")\n",
        "\n",
        "    # 2. RIEMPIMENTO MISSING VALUES\n",
        "    # Testo\n",
        "    text_cols = ['company_profile', 'description', 'requirements', 'benefits']\n",
        "    for col in text_cols:\n",
        "        df_clean[col] = df_clean[col].fillna('')\n",
        "\n",
        "    # Categorie (Aggiungiamo 'country' alla lista)\n",
        "    category_cols = ['department', 'employment_type', 'required_experience',\n",
        "                     'required_education', 'industry', 'function', 'country']\n",
        "\n",
        "    for col in category_cols:\n",
        "        df_clean[col] = df_clean[col].fillna('Unknown')\n",
        "\n",
        "    print(\"‚úì Missing values riempiti (Strategy: 'Unknown' placeholder)\")\n",
        "\n",
        "    # 3. PULIZIA TESTO (Data Cleaning Fondamentale per NLP)\n",
        "    def clean_text(text):\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "        text = text.lower() # Minuscolo\n",
        "        text = re.sub(r'<.*?>', '', text) # Rimuove tag HTML (<br>, <div>)\n",
        "        text = re.sub(r'http\\S+', '', text) # Rimuove URL\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text) # Rimuove numeri e punteggiatura\n",
        "        text = re.sub(r'\\s+', ' ', text).strip() # Rimuove spazi doppi\n",
        "        return text\n",
        "\n",
        "    print(\"Cleaning testo in corso (rimozione HTML, URL, caratteri speciali)...\")\n",
        "    for col in text_cols:\n",
        "        df_clean[col] = df_clean[col].apply(clean_text)\n",
        "    print(\"‚úì Testo pulito\")\n",
        "\n",
        "    # 4. ESTRAZIONE SALARY (Invariato - Era gi√† ottimo)\n",
        "    def extract_salary_info(salary_str):\n",
        "        if pd.isna(salary_str) or salary_str == '':\n",
        "            return 0, 0\n",
        "        try:\n",
        "            # Cerca pattern numerici\n",
        "            nums = re.findall(r'\\d+', str(salary_str))\n",
        "            if len(nums) >= 2:\n",
        "                return float(nums[0]), float(nums[1])\n",
        "            elif len(nums) == 1:\n",
        "                return float(nums[0]), float(nums[0])\n",
        "        except:\n",
        "            pass\n",
        "        return 0, 0\n",
        "\n",
        "    df_clean[['salary_min', 'salary_max']] = df_clean['salary_range'].apply(\n",
        "        lambda x: pd.Series(extract_salary_info(x))\n",
        "    )\n",
        "    df_clean['salary_range_flag'] = (df_clean['salary_range'].notna() & (df_clean['salary_range'] != '')).astype(int)\n",
        "\n",
        "    # Drop colonne inutili\n",
        "    df_clean = df_clean.drop(['job_id', 'salary_range'], axis=1)\n",
        "\n",
        "    print(\"‚úì Features salary estratte\")\n",
        "    print(f\"Dataset shape dopo cleaning: {df_clean.shape}\")\n",
        "\n",
        "    return df_clean"
      ],
      "metadata": {
        "id": "7jINbVWBWtkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. FEATURE ENGINEERING + GRAFICI OUTLIER"
      ],
      "metadata": {
        "id": "fxX-eOJ1WwBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_engineering(df_clean):\n",
        "    \"\"\"Engineering di nuove features\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 3: FEATURE ENGINEERING\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # 1. CREAZIONE META-FEATURES TESTUALI\n",
        "    # Combinazione testo per TF-IDF (che useremo dopo)\n",
        "    df_clean['combined_text'] = (\n",
        "        df_clean['description'] + ' ' +\n",
        "        df_clean['requirements'] + ' ' +\n",
        "        df_clean['benefits']\n",
        "    )\n",
        "\n",
        "    # Lunghezze e conteggi\n",
        "    df_clean['len_description'] = df_clean['description'].str.len()\n",
        "    df_clean['len_requirements'] = df_clean['requirements'].str.len()\n",
        "    df_clean['len_benefits'] = df_clean['benefits'].str.len()\n",
        "    df_clean['len_company_profile'] = df_clean['company_profile'].str.len()\n",
        "\n",
        "    df_clean['words_description'] = df_clean['description'].str.split().str.len()\n",
        "    df_clean['words_requirements'] = df_clean['requirements'].str.split().str.len()\n",
        "\n",
        "    print(\"‚úì Meta-features testuali create\")\n",
        "\n",
        "    # 2. GESTIONE OUTLIERS (Log Transformation)\n",
        "    # Le distribuzioni di lunghezza e salario sono \"skewed\" (coda lunga).\n",
        "    # Applichiamo np.log1p (log(1+x)) per normalizzare la distribuzione e ridurre l'impatto degli outlier.\n",
        "    numeric_features = ['len_description', 'len_requirements', 'len_benefits',\n",
        "                       'len_company_profile', 'words_description', 'words_requirements',\n",
        "                       'salary_min', 'salary_max']\n",
        "\n",
        "    print(\"\\nApplicazione Log-Transformation per gestire gli Outliers...\")\n",
        "    for col in numeric_features:\n",
        "        # Creiamo nuove colonne logaritmiche\n",
        "        df_clean[f'log_{col}'] = np.log1p(df_clean[col])\n",
        "\n",
        "    # Aggiorniamo la lista delle features numeriche da usare nel modello\n",
        "    # Usiamo le versioni logaritmiche che sono pi√π stabili\n",
        "    final_numeric_features = [f'log_{col}' for col in numeric_features]\n",
        "\n",
        "    # GRAFICO 3: Boxplot (Originali vs Log) - Dimostrazione Efficacia\n",
        "    # Mostriamo solo i primi 4 per brevit√† nel grafico, ma li trasformiamo tutti\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    cols_to_plot = ['len_description', 'salary_max'] # Esempio significativo\n",
        "\n",
        "    # Plot Originale vs Log per Description\n",
        "    sns.boxplot(x=df_clean['len_description'], ax=axes[0,0], color='skyblue')\n",
        "    axes[0,0].set_title('Original: Len Description (Many Outliers)', fontweight='bold')\n",
        "\n",
        "    sns.boxplot(x=df_clean['log_len_description'], ax=axes[0,1], color='lightgreen')\n",
        "    axes[0,1].set_title('Log-Transformed: Len Description (Normalized)', fontweight='bold')\n",
        "\n",
        "    # Plot Originale vs Log per Salary\n",
        "    sns.boxplot(x=df_clean['salary_max'], ax=axes[1,0], color='salmon')\n",
        "    axes[1,0].set_title('Original: Salary Max', fontweight='bold')\n",
        "\n",
        "    sns.boxplot(x=df_clean['log_salary_max'], ax=axes[1,1], color='orange')\n",
        "    axes[1,1].set_title('Log-Transformed: Salary Max', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('03_outliers_management.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"‚úì Grafico salvato: 03_outliers_management.png (Confronto Before/After)\")\n",
        "    plt.close()\n",
        "\n",
        "    # GRAFICO 4: Istogrammi (Sulle features trasformate)\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for idx, col in enumerate(final_numeric_features):\n",
        "        sns.histplot(data=df_clean, x=col, hue='fraudulent', kde=True, ax=axes[idx], element=\"step\")\n",
        "        axes[idx].set_title(f'Dist: {col}', fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('04_features_distribution.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"‚úì Grafico salvato: 04_features_distribution.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # 3. ENCODING CATEGORICO\n",
        "    # Aggiunto 'country' che mancava\n",
        "    categorical_features = ['employment_type', 'required_experience',\n",
        "                           'required_education', 'industry', 'function', 'country']\n",
        "\n",
        "    print(f\"\\nEncoding Categorico ({len(categorical_features)} features)...\")\n",
        "    le_dict = {}\n",
        "    for col in categorical_features:\n",
        "        # Usiamo LabelEncoder.\n",
        "        # NOTA PER IL REPORT: Scelta ottimizzata per modelli Tree-based (Random Forest).\n",
        "        # Per modelli lineari puri sarebbe meglio OneHotEncoder, ma aumenterebbe troppo la dimensionalit√† qui.\n",
        "        le = LabelEncoder()\n",
        "        df_clean[f'{col}_encoded'] = le.fit_transform(df_clean[col].astype(str))\n",
        "        le_dict[col] = le\n",
        "\n",
        "    print(f\"‚úì Features categoriche encode completato\")\n",
        "\n",
        "    return df_clean, categorical_features, final_numeric_features"
      ],
      "metadata": {
        "id": "i141mWjsWyWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. PREPARAZIONE DATI PER MODELLAZIONE"
      ],
      "metadata": {
        "id": "jqkXhwnXW2oY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_features(df_clean, categorical_features, numeric_features, test_size=0.2):\n",
        "    \"\"\"Preparazione features finali\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 4: PREPARAZIONE DATI PER MODELLAZIONE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Separazione X e y\n",
        "    X_text = df_clean[['combined_text']]\n",
        "\n",
        "    # Feature binarie che non necessitano di scaling\n",
        "    binary_features = ['telecommuting', 'has_company_logo', 'has_questions', 'salary_range_flag']\n",
        "\n",
        "    # Feature categoriche gi√† encodate\n",
        "    meta_categorical_encoded = [f'{col}_encoded' for col in categorical_features]\n",
        "\n",
        "    # Uniamo le feature numeriche (log-trasformate) e quelle categoriche/binarie\n",
        "    X_meta = df_clean[numeric_features + binary_features + meta_categorical_encoded]\n",
        "    y = df_clean['fraudulent']\n",
        "\n",
        "    # Stratified Train-Test Split (CRUCIALE per dati sbilanciati)\n",
        "    print(\"Esecuzione Stratified Split (mantiene la % di frodi uguale tra train e test)...\")\n",
        "    X_text_train, X_text_test, y_train, y_test = train_test_split(\n",
        "        X_text, y, test_size=test_size, random_state=42, stratify=y\n",
        "    )\n",
        "    # Facciamo lo split parallelo per i metadati usando gli stessi indici\n",
        "    X_meta_train, X_meta_test, _, _ = train_test_split(\n",
        "        X_meta, y, test_size=test_size, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    print(f\"Train set: {len(X_text_train)} samples ({(y_train==0).sum()} real, {(y_train==1).sum()} fraud)\")\n",
        "    print(f\"Test set: {len(X_text_test)} samples ({(y_test==0).sum()} real, {(y_test==1).sum()} fraud)\")\n",
        "\n",
        "    # GRAFICO 5: Train-Test Split Distribution\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    train_dist = y_train.value_counts()\n",
        "    test_dist = y_test.value_counts()\n",
        "\n",
        "    x = ['Real', 'Fraudulent']\n",
        "    train_vals = [train_dist[0], train_dist[1]]\n",
        "    test_vals = [test_dist[0], test_dist[1]]\n",
        "\n",
        "    x_pos = np.arange(len(x))\n",
        "    width = 0.35\n",
        "\n",
        "    axes[0].bar(x_pos - width/2, train_vals, width, label='Train', color='skyblue')\n",
        "    axes[0].bar(x_pos + width/2, test_vals, width, label='salmon')\n",
        "    axes[0].set_ylabel('Count')\n",
        "    axes[0].set_title('Train-Test Split Distribution (Absolute)', fontweight='bold')\n",
        "    axes[0].set_xticks(x_pos)\n",
        "    axes[0].set_xticklabels(x)\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Percentages\n",
        "    train_pct = [train_vals[0]/sum(train_vals)*100, train_vals[1]/sum(train_vals)*100]\n",
        "    test_pct = [test_vals[0]/sum(test_vals)*100, test_vals[1]/sum(test_vals)*100]\n",
        "\n",
        "    axes[1].bar(x_pos - width/2, train_pct, width, label='Train %', color='skyblue')\n",
        "    axes[1].bar(x_pos + width/2, test_pct, width, label='Test %', color='salmon')\n",
        "    axes[1].set_ylabel('Percentage (%)')\n",
        "    axes[1].set_title('Validation of Stratification', fontweight='bold')\n",
        "    axes[1].set_xticks(x_pos)\n",
        "    axes[1].set_xticklabels(x)\n",
        "    axes[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('05_train_test_split.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\n‚úì Grafico salvato: 05_train_test_split.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # TF-IDF Vectorization\n",
        "    print(\"\\nTF-IDF Vectorization...\")\n",
        "    # NOTA: Aumentiamo max_features a 5000. 100 √® troppo poco per catturare frodi.\n",
        "    # Usiamo ngram_range=(1,2) per catturare coppie di parole (es. \"wire transfer\")\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=5000,\n",
        "        stop_words='english',\n",
        "        ngram_range=(1, 2),\n",
        "        min_df=5,\n",
        "        max_df=0.7\n",
        "    )\n",
        "\n",
        "    X_text_train_tfidf = vectorizer.fit_transform(X_text_train['combined_text'])\n",
        "    X_text_test_tfidf = vectorizer.transform(X_text_test['combined_text'])\n",
        "    print(f\"‚úì TF-IDF features created: {X_text_train_tfidf.shape[1]} features\")\n",
        "\n",
        "    # GRAFICO 6: Top TF-IDF Features\n",
        "    tfidf_feature_names = vectorizer.get_feature_names_out()\n",
        "    feature_importance_tfidf = np.asarray(X_text_train_tfidf.mean(axis=0)).ravel()\n",
        "    # Prendiamo le top 20\n",
        "    idx = feature_importance_tfidf.argsort()[-20:][::-1]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    sns.barplot(x=feature_importance_tfidf[idx], y=tfidf_feature_names[idx],\n",
        "                palette='viridis', ax=ax)\n",
        "    ax.set_title('Top 20 Most Frequent Words (TF-IDF Importance)', fontweight='bold')\n",
        "    ax.set_xlabel('Average TF-IDF Score')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('06_tfidf_features.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"‚úì Grafico salvato: 06_tfidf_features.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # SCALING (RobustScaler invece di Standard)\n",
        "    print(\"Scaling meta-features (Using RobustScaler for Outlier Resistance)...\")\n",
        "    # RobustScaler usa mediana e IQR, quindi √® immune agli outlier che abbiamo visto nei boxplot\n",
        "    scaler = RobustScaler()\n",
        "\n",
        "    # Scaliamo solo le numeriche continue, non le binarie o categoriche encodate\n",
        "    # (Anche se tecnicamente scalare le binarie non rompe nulla, √® pi√π pulito cos√¨)\n",
        "    X_meta_train_scaled = scaler.fit_transform(X_meta_train)\n",
        "    X_meta_test_scaled = scaler.transform(X_meta_test)\n",
        "\n",
        "    print(f\"‚úì Meta-features scaled: {X_meta_train_scaled.shape[1]} features\")\n",
        "\n",
        "    # Concatenazione features\n",
        "    print(\"\\nConcatenating Text + Meta features...\")\n",
        "    X_train = hstack([X_text_train_tfidf, csr_matrix(X_meta_train_scaled)])\n",
        "    X_test = hstack([X_text_test_tfidf, csr_matrix(X_meta_test_scaled)])\n",
        "\n",
        "    # Conversione a Dense (Attenzione alla RAM se max_features > 10000)\n",
        "    # Con 5000 features √® gestibile e facilita il debug\n",
        "    X_train_dense = X_train.toarray()\n",
        "    X_test_dense = X_test.toarray()\n",
        "\n",
        "    print(f\"‚úì Final Dataset Shape: {X_train_dense.shape}\")\n",
        "    print(f\"  (Includes 5000 TF-IDF tokens + {X_meta_train_scaled.shape[1]} meta-features)\")\n",
        "\n",
        "    return (X_train_dense, X_test_dense, y_train, y_test, vectorizer, scaler,\n",
        "            X_text_train_tfidf, X_text_test_tfidf, tfidf_feature_names)"
      ],
      "metadata": {
        "id": "Nas2UCdXW5SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. TRAINING E VALUTAZIONE MODELLI"
      ],
      "metadata": {
        "id": "Syma39FeW-fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
        "    \"\"\"Addestramento e valutazione modello\"\"\"\n",
        "    print(f\"Training {model_name}...\", end=\" \", flush=True)\n",
        "\n",
        "    # Training\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Prediction (Soglia standard 0.5)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Probabilities (Gestione eccezione per modelli che non hanno predict_proba, es. SVM hinge)\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    else:\n",
        "        # Per SVM con loss='hinge', usiamo decision_function\n",
        "        y_pred_proba = model.decision_function(X_test)\n",
        "        # Normalizziamo per avere pseudo-probabilit√† per ROC-AUC\n",
        "        y_pred_proba = (y_pred_proba - y_pred_proba.min()) / (y_pred_proba.max() - y_pred_proba.min())\n",
        "\n",
        "    # Metriche\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "    print(f\"‚úì F1: {f1:.4f} | Recall: {rec:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'model_name': model_name,\n",
        "        'accuracy': acc,\n",
        "        'precision': prec,\n",
        "        'recall': rec,\n",
        "        'f1': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'model': model,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba,\n",
        "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
        "        'classification_report': classification_report(y_test, y_pred)\n",
        "    }\n",
        "\n",
        "def train_models(X_train_dense, X_test_dense, y_train, y_test):\n",
        "    \"\"\"Training di pi√π modelli (Logistic, RF, SVM)\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 5: ADDESTRAMENTO E VALUTAZIONE MODELLI\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    models = {\n",
        "        # 1. BASELINE LINEARE\n",
        "        'Logistic Regression': LogisticRegression(\n",
        "            max_iter=1000,\n",
        "            random_state=42,\n",
        "            class_weight='balanced', # Fondamentale per sbilanciamento\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "\n",
        "        # 2. ENSEMBLE (Pi√π potente)\n",
        "        'Random Forest': RandomForestClassifier(\n",
        "            n_estimators=100,      # Aumentato da 50 a 100\n",
        "            random_state=42,\n",
        "            class_weight='balanced',\n",
        "            n_jobs=-1,\n",
        "            max_depth=None         # RIMOSSO IL CAP A 10! Lasciamolo imparare.\n",
        "        ),\n",
        "\n",
        "        # 3. SVM LINEARE (Ottimo per NLP e alta dimensionalit√†)\n",
        "        'Linear SVM (SGD)': SGDClassifier(\n",
        "            loss='log_loss',       # 'log_loss' d√† probabilit√† (equivalente a Logistic Regression ma pi√π veloce su big data)\n",
        "            random_state=42,\n",
        "            class_weight='balanced',\n",
        "            max_iter=1000,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "    for name, model in models.items():\n",
        "        result = evaluate_model(model, X_train_dense, X_test_dense, y_train, y_test, name)\n",
        "        results.append(result)\n",
        "\n",
        "    # Summary Table\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"MODELLI COMPARISON TABLE\")\n",
        "    print(\"=\"*80)\n",
        "    summary = pd.DataFrame([\n",
        "        {\n",
        "            'Model': r['model_name'],\n",
        "            'Accuracy': r['accuracy'],\n",
        "            'Precision': r['precision'],\n",
        "            'Recall': r['recall'],\n",
        "            'F1-Score': r['f1'],\n",
        "            'ROC-AUC': r['roc_auc']\n",
        "        } for r in results\n",
        "    ]).sort_values(by='F1-Score', ascending=False)\n",
        "\n",
        "    print(summary.to_string(index=False, float_format=\"%.4f\"))\n",
        "\n",
        "    # GRAFICO 7: Model Comparison\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6)) # Semplificato in 1 riga\n",
        "\n",
        "    # Plot 1: F1-Score (Il pi√π importante)\n",
        "    sns.barplot(x='F1-Score', y='Model', data=summary, ax=axes[0], palette='viridis')\n",
        "    axes[0].set_title('Model F1-Score (The Key Metric)', fontweight='bold')\n",
        "    axes[0].set_xlim(0, 1)\n",
        "\n",
        "    # Plot 2: Recall (Per non perdere frodi)\n",
        "    sns.barplot(x='Recall', y='Model', data=summary, ax=axes[1], palette='magma')\n",
        "    axes[1].set_title('Model Recall (Sensitivity)', fontweight='bold')\n",
        "    axes[1].set_xlim(0, 1)\n",
        "\n",
        "    # Plot 3: ROC-AUC\n",
        "    sns.barplot(x='ROC-AUC', y='Model', data=summary, ax=axes[2], palette='coolwarm')\n",
        "    axes[2].set_title('Model ROC-AUC', fontweight='bold')\n",
        "    axes[2].set_xlim(0.5, 1) # AUC parte da 0.5 (random)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('07_models_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"\\n‚úì Grafico salvato: 07_models_comparison.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Selezione Miglior Modello\n",
        "    best_result = max(results, key=lambda x: x['f1'])\n",
        "    print(f\"\\nüèÜ BEST MODEL: {best_result['model_name']} con F1 = {best_result['f1']:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "    return results, best_result"
      ],
      "metadata": {
        "id": "lidddBhtW9hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. ANALISI DETTAGLIATA + GRAFICI"
      ],
      "metadata": {
        "id": "9JQMCgFlXHFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detailed_analysis(best_result, vectorizer, feature_names_meta, y_test):\n",
        "    \"\"\"Analisi dettagliata del miglior modello con Ottimizzazione Soglia\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"STEP 6: ANALISI DETTAGLIATA - \" + best_result['model_name'].upper())\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # 1. ANALISI SOGLIA OTTIMALE (Threshold Tuning)\n",
        "    # Le frodi sono rare. Spesso la soglia 0.5 non √® l'ideale.\n",
        "    # Cerchiamo la soglia che massimizza l'F1-Score.\n",
        "    print(\"\\n--- THRESHOLD TUNING ---\")\n",
        "    y_proba = best_result['y_pred_proba']\n",
        "\n",
        "    thresholds = np.arange(0.1, 1.0, 0.05)\n",
        "    best_thresh = 0.5\n",
        "    best_f1 = 0\n",
        "    scores = []\n",
        "\n",
        "    for t in thresholds:\n",
        "        preds = (y_proba >= t).astype(int)\n",
        "        f1 = f1_score(y_test, preds)\n",
        "        rec = recall_score(y_test, preds)\n",
        "        prec = precision_score(y_test, preds, zero_division=0)\n",
        "        scores.append({'threshold': t, 'f1': f1, 'recall': rec, 'precision': prec})\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_thresh = t\n",
        "\n",
        "    print(f\"Soglia Standard (0.50) -> F1: {best_result['f1']:.4f}\")\n",
        "    print(f\"Soglia Ottimale ({best_thresh:.2f}) -> F1: {best_f1:.4f} (Miglioramento: {best_f1 - best_result['f1']:.4f})\")\n",
        "\n",
        "    # Aggiorniamo le predizioni con la nuova soglia per i grafici successivi\n",
        "    y_pred_opt = (y_proba >= best_thresh).astype(int)\n",
        "\n",
        "    # 2. CONFUSION MATRIX (Con soglia ottimizzata)\n",
        "    cm = confusion_matrix(y_test, y_pred_opt)\n",
        "\n",
        "    # Calcoli manuali per leggibilit√†\n",
        "    TN, FP, FN, TP = cm.ravel() # Flattening 2x2 matrix\n",
        "\n",
        "    print(f\"\\nConfusion Matrix Analysis (Threshold {best_thresh:.2f}):\")\n",
        "    print(f\"  True Positives (Fraud detected): {TP}\")\n",
        "    print(f\"  True Negatives (Real identified): {TN}\")\n",
        "    print(f\"  False Positives (Real -> Fraud): {FP} (Falsi Allarmi)\")\n",
        "    print(f\"  False Negatives (Fraud missed): {FN} (Truffe perse - IL DATO PI√ô CRITICO)\")\n",
        "\n",
        "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
        "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0 # Recall\n",
        "\n",
        "    print(f\"\\nMetriche Finali:\")\n",
        "    print(f\"  Specificity: {specificity:.4f}\")\n",
        "    print(f\"  Recall (Sensitivity): {sensitivity:.4f}\")\n",
        "    print(f\"  Precision: {TP / (TP + FP):.4f}\")\n",
        "\n",
        "    # GRAFICO 8: Confusion Matrix\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
        "                xticklabels=['Real (0)', 'Fraudulent (1)'],\n",
        "                yticklabels=['Real (0)', 'Fraudulent (1)'])\n",
        "    ax.set_title(f'Confusion Matrix (Best F1 Threshold: {best_thresh:.2f})', fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('08_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"‚úì Grafico salvato: 08_confusion_matrix.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # GRAFICO 9: ROC Curve\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
        "    ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title('ROC Curve', fontweight='bold')\n",
        "    ax.legend(loc=\"lower right\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('09_roc_curve.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"‚úì Grafico salvato: 09_roc_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # GRAFICO 10: Precision-Recall Curve\n",
        "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    ax.plot(recall, precision, color='purple', lw=2, label='PR Curve')\n",
        "    ax.set_xlabel('Recall')\n",
        "    ax.set_ylabel('Precision')\n",
        "    ax.set_title('Precision-Recall Curve', fontweight='bold')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('10_precision_recall_curve.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"‚úì Grafico salvato: 10_precision_recall_curve.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # 3. FEATURE IMPORTANCE\n",
        "    # Ricostruzione nomi features\n",
        "    print(f\"\\nCalcolo Feature Importance...\")\n",
        "    try:\n",
        "        # Recupera nomi dal vectorizer (5000 features)\n",
        "        tfidf_names = list(vectorizer.get_feature_names_out())\n",
        "        # Unisce con i nomi delle meta-features\n",
        "        all_features = tfidf_names + feature_names_meta\n",
        "\n",
        "        # Estrazione coefficienti/importanza\n",
        "        if hasattr(best_result['model'], 'coef_'):\n",
        "            # Modelli Lineari (Logistic/SVM)\n",
        "            importances = best_result['model'].coef_[0]\n",
        "            imp_df = pd.DataFrame({'Feature': all_features, 'Value': importances})\n",
        "            imp_df['Abs_Value'] = imp_df['Value'].abs()\n",
        "            imp_df = imp_df.sort_values('Abs_Value', ascending=False).head(20)\n",
        "\n",
        "            # Plot Colore Divergente (Rosso = Indicatore Frode, Verde = Indicatore Legit)\n",
        "            colors = ['red' if x > 0 else 'green' for x in imp_df['Value']] # Assumendo 1=Fraud\n",
        "            title = f\"Top 20 Features - {best_result['model_name']} (Red=Fraud-like, Green=Real-like)\"\n",
        "\n",
        "        elif hasattr(best_result['model'], 'feature_importances_'):\n",
        "            # Random Forest\n",
        "            importances = best_result['model'].feature_importances_\n",
        "            imp_df = pd.DataFrame({'Feature': all_features, 'Value': importances})\n",
        "            imp_df = imp_df.sort_values('Value', ascending=False).head(20)\n",
        "            colors = 'skyblue'\n",
        "            title = f\"Top 20 Feature Importance - {best_result['model_name']}\"\n",
        "\n",
        "        else:\n",
        "            print(\"Il modello non supporta feature importance diretta.\")\n",
        "            return\n",
        "\n",
        "        print(imp_df[['Feature', 'Value']].to_string(index=False))\n",
        "\n",
        "        # GRAFICO 11\n",
        "        fig, ax = plt.subplots(figsize=(12, 8))\n",
        "        ax.barh(imp_df['Feature'], imp_df['Value'], color=colors)\n",
        "        ax.set_title(title, fontweight='bold')\n",
        "        ax.invert_yaxis() # Top feature in alto\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('11_feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "        print(\"‚úì Grafico salvato: 11_feature_importance.png\")\n",
        "        plt.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Errore nel plot Feature Importance: {e}\")\n",
        "        print(\"Probabile causa: mismatch tra numero features TF-IDF e lista nomi.\")"
      ],
      "metadata": {
        "id": "Bmrk4YxIXII2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MAIN PIPELINE"
      ],
      "metadata": {
        "id": "kEL5XLrkXSbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(filepath='fake_job_postings.csv'):\n",
        "    \"\"\"Pipeline completa di Machine Learning\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"FRAUD JOB POSTINGS DETECTION - PIPELINE DI PROGETTO\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Verifica esistenza file\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"‚ùå ERRORE: Il file '{filepath}' non √® stato trovato nella cartella corrente.\")\n",
        "        print(\"   Assicurati di aver scaricato il dataset e rinominato il file correttamente.\")\n",
        "        return None\n",
        "\n",
        "    # Step 1: Load & Explore\n",
        "    df = load_and_explore_data(filepath)\n",
        "\n",
        "    # Step 2: Clean\n",
        "    df_clean = clean_data(df)\n",
        "\n",
        "    # Step 3: Feature Engineering\n",
        "    # Restituisce il df, le categorie originali e le numeriche (log-trasformate)\n",
        "    df_clean, cat_features, num_features = feature_engineering(df_clean)\n",
        "\n",
        "    # Step 4: Prepare Features\n",
        "    # Qui avviene la magia delle matrici. √à fondamentale che i nomi corrispondano.\n",
        "    (X_train, X_test, y_train, y_test, vectorizer, scaler,\n",
        "     X_text_train, X_text_test, tfidf_feature_names) = prepare_features(\n",
        "        df_clean, cat_features, num_features\n",
        "    )\n",
        "\n",
        "    # Step 5: Train Models (Include Threshold Tuning)\n",
        "    results, best_result = train_models(X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # --- COSTRUZIONE NOMI FEATURES PER IL REPORT ---\n",
        "    # ATTENZIONE: L'ordine deve rispecchiare ESATTAMENTE l'hstack fatto nello Step 4.\n",
        "    # Ordine Step 4: [TF-IDF] + [Numeric] + [Binary] + [Categorical Encoded]\n",
        "\n",
        "    binary_features = ['telecommuting', 'has_company_logo', 'has_questions', 'salary_range_flag']\n",
        "    encoded_features = [f'{col}_encoded' for col in cat_features]\n",
        "\n",
        "    # Lista completa dei nomi dei metadati (senza TF-IDF che viene aggiunto dentro la funzione detailed_analysis)\n",
        "    meta_features_names = num_features + binary_features + encoded_features\n",
        "\n",
        "    # Step 6: Detailed Analysis & Visualization\n",
        "    detailed_analysis(best_result, vectorizer, meta_features_names, y_test)\n",
        "\n",
        "    # Chiusura\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"PIPELINE COMPLETATA IN {elapsed_time:.2f} SECONDI\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\n‚úì File generati nella cartella:\")\n",
        "    grafici = [\n",
        "        \"01_class_distribution.png\", \"02_missing_values.png\", \"03_outliers_management.png\",\n",
        "        \"04_features_distribution.png\", \"05_train_test_split.png\", \"06_tfidf_features.png\",\n",
        "        \"07_models_comparison.png\", \"08_confusion_matrix.png\", \"09_roc_curve.png\",\n",
        "        \"10_precision_recall_curve.png\", \"11_feature_importance.png\"\n",
        "    ]\n",
        "    for g in grafici:\n",
        "        print(f\"  üìÑ {g}\")\n",
        "\n",
        "    return {\n",
        "        'dataset': df_clean,\n",
        "        'models': results,\n",
        "        'best_model': best_result,\n",
        "        'vectorizer': vectorizer,\n",
        "        'scaler': scaler\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Avvio Pipeline\n",
        "    pipeline_results = main('fake_job_postings.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZjWyHwfXVcC",
        "outputId": "2be127e4-25f4-47c6-a65d-1605b760d27b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "FRAUD JOB POSTINGS DETECTION - PIPELINE DI PROGETTO\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "STEP 1: CARICAMENTO E ESPLORAZIONE DATASET\n",
            "================================================================================\n",
            "\n",
            "Dimensioni dataset: (17880, 18)\n",
            "\n",
            "Info Dataset (Tipi di dato):\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 17880 entries, 0 to 17879\n",
            "Data columns (total 18 columns):\n",
            " #   Column               Non-Null Count  Dtype \n",
            "---  ------               --------------  ----- \n",
            " 0   job_id               17880 non-null  int64 \n",
            " 1   title                17880 non-null  object\n",
            " 2   location             17534 non-null  object\n",
            " 3   department           6333 non-null   object\n",
            " 4   salary_range         2868 non-null   object\n",
            " 5   company_profile      14572 non-null  object\n",
            " 6   description          17879 non-null  object\n",
            " 7   requirements         15184 non-null  object\n",
            " 8   benefits             10668 non-null  object\n",
            " 9   telecommuting        17880 non-null  int64 \n",
            " 10  has_company_logo     17880 non-null  int64 \n",
            " 11  has_questions        17880 non-null  int64 \n",
            " 12  employment_type      14409 non-null  object\n",
            " 13  required_experience  10830 non-null  object\n",
            " 14  required_education   9775 non-null   object\n",
            " 15  industry             12977 non-null  object\n",
            " 16  function             11425 non-null  object\n",
            " 17  fraudulent           17880 non-null  int64 \n",
            "dtypes: int64(5), object(13)\n",
            "memory usage: 2.5+ MB\n",
            "None\n",
            "\n",
            "‚úì Nessun duplicato trovato.\n",
            "\n",
            "Distribuzione target:\n",
            "fraudulent\n",
            "0    17014\n",
            "1      866\n",
            "Name: count, dtype: int64\n",
            "Class Imbalance: 4.84% fraudulent (SBILANCIATO)\n",
            "\n",
            "Valori nulli per colonna (%):\n",
            "salary_range           83.959732\n",
            "department             64.580537\n",
            "required_education     45.329978\n",
            "benefits               40.335570\n",
            "required_experience    39.429530\n",
            "function               36.101790\n",
            "industry               27.421700\n",
            "employment_type        19.412752\n",
            "company_profile        18.501119\n",
            "requirements           15.078300\n",
            "location                1.935123\n",
            "description             0.005593\n",
            "dtype: float64\n",
            "\n",
            "‚úì Grafico salvato: 01_class_distribution.png\n",
            "‚úì Grafico salvato: 02_missing_values.png\n",
            "\n",
            "================================================================================\n",
            "STEP 2: GESTIONE MISSING VALUES E DATA CLEANING\n",
            "================================================================================\n",
            "‚úì Feature 'country' estratta dalla location\n",
            "‚úì Missing values riempiti (Strategy: 'Unknown' placeholder)\n",
            "Cleaning testo in corso (rimozione HTML, URL, caratteri speciali)...\n",
            "‚úì Testo pulito\n",
            "‚úì Features salary estratte\n",
            "Dataset shape dopo cleaning: (17880, 19)\n",
            "\n",
            "================================================================================\n",
            "STEP 3: FEATURE ENGINEERING\n",
            "================================================================================\n",
            "‚úì Meta-features testuali create\n",
            "\n",
            "Applicazione Log-Transformation per gestire gli Outliers...\n",
            "‚úì Grafico salvato: 03_outliers_management.png (Confronto Before/After)\n",
            "‚úì Grafico salvato: 04_features_distribution.png\n",
            "\n",
            "Encoding Categorico (6 features)...\n",
            "‚úì Features categoriche encode completato\n",
            "\n",
            "================================================================================\n",
            "STEP 4: PREPARAZIONE DATI PER MODELLAZIONE\n",
            "================================================================================\n",
            "Esecuzione Stratified Split (mantiene la % di frodi uguale tra train e test)...\n",
            "Train set: 14304 samples (13611 real, 693 fraud)\n",
            "Test set: 3576 samples (3403 real, 173 fraud)\n",
            "\n",
            "‚úì Grafico salvato: 05_train_test_split.png\n",
            "\n",
            "TF-IDF Vectorization...\n",
            "‚úì TF-IDF features created: 5000 features\n",
            "‚úì Grafico salvato: 06_tfidf_features.png\n",
            "Scaling meta-features (Using RobustScaler for Outlier Resistance)...\n",
            "‚úì Meta-features scaled: 18 features\n",
            "\n",
            "Concatenating Text + Meta features...\n",
            "‚úì Final Dataset Shape: (14304, 5018)\n",
            "  (Includes 5000 TF-IDF tokens + 18 meta-features)\n",
            "\n",
            "================================================================================\n",
            "STEP 5: ADDESTRAMENTO E VALUTAZIONE MODELLI\n",
            "================================================================================\n",
            "Training Logistic Regression... ‚úì F1: 0.6482 | Recall: 0.8786\n",
            "Training Random Forest... ‚úì F1: 0.7041 | Recall: 0.5434\n",
            "Training Linear SVM (SGD)... ‚úì F1: 0.5882 | Recall: 0.8960\n",
            "\n",
            "================================================================================\n",
            "MODELLI COMPARISON TABLE\n",
            "================================================================================\n",
            "              Model  Accuracy  Precision  Recall  F1-Score  ROC-AUC\n",
            "      Random Forest    0.9779     1.0000  0.5434    0.7041   0.9884\n",
            "Logistic Regression    0.9539     0.5135  0.8786    0.6482   0.9814\n",
            "   Linear SVM (SGD)    0.9393     0.4379  0.8960    0.5882   0.9770\n",
            "\n",
            "‚úì Grafico salvato: 07_models_comparison.png\n",
            "\n",
            "üèÜ BEST MODEL: Random Forest con F1 = 0.7041\n",
            "\n",
            "================================================================================\n",
            "STEP 6: ANALISI DETTAGLIATA - RANDOM FOREST\n",
            "================================================================================\n",
            "\n",
            "--- THRESHOLD TUNING ---\n",
            "Soglia Standard (0.50) -> F1: 0.7041\n",
            "Soglia Ottimale (0.20) -> F1: 0.8023 (Miglioramento: 0.0982)\n",
            "\n",
            "Confusion Matrix Analysis (Threshold 0.20):\n",
            "  True Positives (Fraud detected): 140\n",
            "  True Negatives (Real identified): 3367\n",
            "  False Positives (Real -> Fraud): 36 (Falsi Allarmi)\n",
            "  False Negatives (Fraud missed): 33 (Truffe perse - IL DATO PI√ô CRITICO)\n",
            "\n",
            "Metriche Finali:\n",
            "  Specificity: 0.9894\n",
            "  Recall (Sensitivity): 0.8092\n",
            "  Precision: 0.7955\n",
            "‚úì Grafico salvato: 08_confusion_matrix.png\n",
            "‚úì Grafico salvato: 09_roc_curve.png\n",
            "‚úì Grafico salvato: 10_precision_recall_curve.png\n",
            "\n",
            "Calcolo Feature Importance...\n",
            "                   Feature    Value\n",
            "   log_len_company_profile 0.046121\n",
            "          has_company_logo 0.030660\n",
            "           country_encoded 0.013089\n",
            "          function_encoded 0.010640\n",
            "                      team 0.008748\n",
            "                       web 0.007709\n",
            "             has_questions 0.007525\n",
            "required_education_encoded 0.007120\n",
            "                   growing 0.006856\n",
            "    log_words_requirements 0.006752\n",
            "       log_len_description 0.006626\n",
            "      log_len_requirements 0.006617\n",
            "     log_words_description 0.005193\n",
            "                    skills 0.004847\n",
            "                      love 0.004827\n",
            "                data entry 0.004825\n",
            "                      home 0.004783\n",
            "          industry_encoded 0.004589\n",
            "                      role 0.004429\n",
            "                    needed 0.004176\n",
            "‚úì Grafico salvato: 11_feature_importance.png\n",
            "\n",
            "================================================================================\n",
            "PIPELINE COMPLETATA IN 120.02 SECONDI\n",
            "================================================================================\n",
            "\n",
            "‚úì File generati nella cartella:\n",
            "  üìÑ 01_class_distribution.png\n",
            "  üìÑ 02_missing_values.png\n",
            "  üìÑ 03_outliers_management.png\n",
            "  üìÑ 04_features_distribution.png\n",
            "  üìÑ 05_train_test_split.png\n",
            "  üìÑ 06_tfidf_features.png\n",
            "  üìÑ 07_models_comparison.png\n",
            "  üìÑ 08_confusion_matrix.png\n",
            "  üìÑ 09_roc_curve.png\n",
            "  üìÑ 10_precision_recall_curve.png\n",
            "  üìÑ 11_feature_importance.png\n"
          ]
        }
      ]
    }
  ]
}